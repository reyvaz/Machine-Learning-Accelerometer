<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Reynaldo" />


<title>Machine Learning on Accelerometer Data</title>

<script src="barbellRmd_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="barbellRmd_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="barbellRmd_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="barbellRmd_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="barbellRmd_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="barbellRmd_files/navigation-1.1/tabsets.js"></script>
<link href="barbellRmd_files/highlightjs-1.1/default.css" rel="stylesheet" />
<script src="barbellRmd_files/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Machine Learning on Accelerometer Data</h1>
<h4 class="author"><em>Reynaldo</em></h4>
<h4 class="date"><em>5/9/2017</em></h4>

</div>


<div id="summary" class="section level2">
<h2>Summary</h2>
<p>This analysis uses different machine learning algorithms on accelerometer data to predict the way individuals perform weight-lifting exercises. The dataset comes from Veloso et al., (2013) and it contains data from accelerometers on the belt, forearm, arm, and dumbbell from 6 individuals.</p>
<p>The individuals were asked to perform one set of 10 repetitions of the unilateral dumbbell biceps curl in five different ways: (A) correctly; (B) throwing the elbows to the front; (C) lifting the dumbbell only halfway; (D) lowering the dumbbell only halfway; and (E) throwing the hips to the front.</p>
<p>The following analysis tests different machine learning algorithms, including CART, Random Forest, and Boosted (GBM) to predict the way dumbbell biceps curls were performed. The best performing algorithm is a Random Forest specification with <span class="math inline">\(99.3\%\)</span> accuracy, followed by a Boosted GBM with <span class="math inline">\(94.3\%\)</span> accuracy, in the test dataset.</p>
<ol style="list-style-type: decimal">
<li>Necessary packages</li>
</ol>
<pre class="r"><code>library(caret); library(dplyr); library(knitr); library(pander)
library(rpart); library(rpart.plot); library(gbm)
library(randomForest); library(ggRandomForests); library(corrplot)</code></pre>
</div>
<div id="data-processing" class="section level2">
<h2>Data Processing</h2>
<p>The training dataset is available <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">here</a>. And the test dataset is available <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">here</a>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Data</li>
</ol>
<pre class="r"><code>test  &lt;- read.csv(&quot;pml-testing.csv&quot;)
train &lt;- read.csv(&quot;pml-training.csv&quot;)
names(train)
str(train)</code></pre>
<p>The original dataset contains <span class="math inline">\(19622\)</span> observations of <span class="math inline">\(160\)</span> variables. Preliminary analysis found: (a) a large number of variables with near zero variability; (b) the first columns contain recording and identification data irrelevant to the prediction; and (c) a significant number of variables with less than <span class="math inline">\(10\%\)</span> of valid observations. The variables with these characteristics are thus discarded.</p>
<ol start="3" style="list-style-type: decimal">
<li>Discard variables<br />
a. discard variables with near zero variance<br />
b. discard variables irrelevant to prediction (remaining columns 1-6).<br />
c. discard variables with 80% NAs or more</li>
</ol>
<pre class="r"><code>nrzv  &lt;- nearZeroVar(train)
train &lt;- train[,-nrzv]
train &lt;- train[, -c(1:6)]
xNAs  &lt;- which(colMeans(is.na(train)) &gt; .8)
train &lt;- train[, -xNAs]</code></pre>
<p><strong>Dataset Split</strong><br />
Data is then split into a training dataset with 60% of instances for model training, and a testing dataset with the remaining 40% of instances.</p>
<ol start="4" style="list-style-type: decimal">
<li>Split data for training and testing</li>
</ol>
<pre class="r"><code>set.seed(400)
inTrain  = createDataPartition(train$classe, p = 3/5)[[1]]
training = train[ inTrain,]
testing  = train[-inTrain,]</code></pre>
<p><strong>Pre-processing</strong><br />
The training dataset now contains <span class="math inline">\(11776\)</span> observations of <span class="math inline">\(53\)</span> variables, all cells with valid entries. Although correlation analysis shows a high correlation between a number of predictors (as seen in the visualization in the appendix), no further manual pre-processing will be performed for the remaining of the analysis. As a note, PCA decomposition (discarded in this report) was able to capture <span class="math inline">\(95\%\)</span> of the variability by reducing the number of components by over <span class="math inline">\(50\%\)</span>, but accuracy was significantly compromised, while expediency gains were only minor. Because of the nature of decision trees (i.e. they can branch at equivalent splitting points) scaling or translational normalization is not necessary. Furthermore, they are also robust to correlated variates.</p>
</div>
<div id="machine-learning-specifications" class="section level2">
<h2>Machine Learning Specifications</h2>
<p>The following will test and compare the performance of 3 different machine learning algorithms: CART, Random Forest, and GBM.</p>
<ol start="5" style="list-style-type: decimal">
<li>First Model: Classification and Regression Tree (CART)</li>
</ol>
<pre class="r"><code>tm.1      &lt;- system.time(
rpart1    &lt;- rpart(classe ~ ., method=&quot;class&quot;, data=training))
testrpart &lt;- predict(rpart1, newdata = testing[,-length(testing)], type = &quot;class&quot;)
cm1       &lt;- confusionMatrix(testrpart, testing$classe)</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>Second Model: Random Forest (RF)</li>
</ol>
<pre class="r"><code>tm.2    &lt;- system.time(
rfm     &lt;- randomForest(training[,-length(training)], training[,length(training)], ntree = 500))
testrfm &lt;- predict(rfm, newdata = testing[,-length(testing)])
cm2     &lt;- confusionMatrix(testrfm, testing$classe)</code></pre>
<ol start="7" style="list-style-type: decimal">
<li>Third Model: Boosting (GBM)</li>
</ol>
<pre class="r"><code>tm.3      &lt;- system.time(
gbm1      &lt;- gbm.fit(x = training[,-length(training)], y = training[,length(training)],
                        distribution = &quot;multinomial&quot;, verbose = FALSE, 
                        interaction.depth=5, shrinkage=0.005, n.trees = 1000))
best.iter &lt;- gbm.perf(gbm1,method=&quot;OOB&quot;, plot.it = FALSE)
probs     &lt;- predict(gbm1, testing[,-length(testing)], n.trees = best.iter, type = &quot;response&quot;)
indexes   &lt;- apply(probs, 1, which.max)
testgbm   &lt;- colnames(probs)[indexes]
cm3       &lt;- confusionMatrix(testgbm, testing$classe)</code></pre>
</div>
<div id="algorithm-performance-comparison" class="section level2">
<h2>Algorithm Performance Comparison</h2>
<p>The following are some extractions from the Confusion Matrix output for each specification. These calculations were done on the test dataset (except time elapsed) and should be an unbiased estimate of out of sample performance.</p>
<ol start="8" style="list-style-type: decimal">
<li>Performance Comparison</li>
</ol>
<pre class="r"><code>Accuracy     &lt;- as.numeric(c(cm1$overall[1], cm2$overall[1], cm3$overall[1]))
Kappa        &lt;- as.numeric(c(cm1$overall[2], cm2$overall[2], cm3$overall[2]))
OOBError     &lt;- 1 - Accuracy
Time.Elapsed &lt;- as.numeric(c(tm.1[3], tm.2[3], tm.3[3]))
Results      &lt;- rbind(Accuracy, Kappa, OOBError, Time.Elapsed)
colnames(Results) &lt;- c(&quot;CART&quot;, &quot;Random Forest&quot;, &quot;GBM&quot;)</code></pre>
<div id="the-calculated-accuracy-rates-kappas-and-out-of-sample-error-rates-estimates-for-each-specification-are" class="section level4">
<h4>The calculated Accuracy rates, Kappas, and Out-of-Sample Error rates estimates for each specification are:</h4>
<table class="kable_wrapper">
<caption>
<strong>Model Performance</strong>
</caption>
<tbody>
<tr>
<td>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">CART</th>
<th align="right">Random Forest</th>
<th align="right">GBM</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td align="right">0.7243</td>
<td align="right">0.9934</td>
<td align="right">0.9434</td>
</tr>
<tr class="even">
<td>Kappa</td>
<td align="right">0.6496</td>
<td align="right">0.9916</td>
<td align="right">0.9284</td>
</tr>
<tr class="odd">
<td>OOBError</td>
<td align="right">0.2757</td>
<td align="right">0.0066</td>
<td align="right">0.0566</td>
</tr>
<tr class="even">
<td>Time.Elapsed</td>
<td align="right">2.1910</td>
<td align="right">29.2390</td>
<td align="right">211.0840</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>Performance metrics indicate that the Random Forest is the best performing algorithm tested here for this purpose with an accuracy rate of <span class="math inline">\(0.993245\)</span>. It is followed by the GBM algorithm with an accuracy rate of <span class="math inline">\(0.9434107\)</span>. And last, is the CART algorithm which performed poorly compared to the other two with an accuracy rate of <span class="math inline">\(0.7243181\)</span>. In terms of time efficiency, the Random Forest specification is about 7 times faster than GBM.</p>
<p>The following reports the Confusion Matrices for the two best performing algorithms in terms of accuracy. Class Specific statistics are for the Random Forest model only.</p>
<table style="width:46%;">
<caption><strong>Confusion Matrix (GMB)</strong></caption>
<colgroup>
<col width="11%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"> </th>
<th align="center">A</th>
<th align="center">B</th>
<th align="center">C</th>
<th align="center">D</th>
<th align="center">E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>A</strong></td>
<td align="center">2175</td>
<td align="center">75</td>
<td align="center">0</td>
<td align="center">3</td>
<td align="center">12</td>
</tr>
<tr class="even">
<td align="left"><strong>B</strong></td>
<td align="center">30</td>
<td align="center">1384</td>
<td align="center">60</td>
<td align="center">12</td>
<td align="center">29</td>
</tr>
<tr class="odd">
<td align="left"><strong>C</strong></td>
<td align="center">10</td>
<td align="center">55</td>
<td align="center">1279</td>
<td align="center">57</td>
<td align="center">27</td>
</tr>
<tr class="even">
<td align="left"><strong>D</strong></td>
<td align="center">13</td>
<td align="center">3</td>
<td align="center">24</td>
<td align="center">1208</td>
<td align="center">18</td>
</tr>
<tr class="odd">
<td align="left"><strong>E</strong></td>
<td align="center">4</td>
<td align="center">1</td>
<td align="center">5</td>
<td align="center">6</td>
<td align="center">1356</td>
</tr>
</tbody>
</table>
<table style="width:46%;">
<caption><strong>Confusion Matrix (Random Forest)</strong></caption>
<colgroup>
<col width="11%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"> </th>
<th align="center">A</th>
<th align="center">B</th>
<th align="center">C</th>
<th align="center">D</th>
<th align="center">E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>A</strong></td>
<td align="center">2231</td>
<td align="center">8</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="left"><strong>B</strong></td>
<td align="center">1</td>
<td align="center">1504</td>
<td align="center">20</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="left"><strong>C</strong></td>
<td align="center">0</td>
<td align="center">6</td>
<td align="center">1346</td>
<td align="center">12</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="left"><strong>D</strong></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">2</td>
<td align="center">1274</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="left"><strong>E</strong></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">1439</td>
</tr>
</tbody>
</table>
<table class="kable_wrapper">
<caption>
<strong>Statistics By Class (Random Forest)</strong>
</caption>
<tbody>
<tr>
<td>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">Class: A</th>
<th align="right">Class: B</th>
<th align="right">Class: C</th>
<th align="right">Class: D</th>
<th align="right">Class: E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sensitivity</td>
<td align="right">0.9996</td>
<td align="right">0.9908</td>
<td align="right">0.9839</td>
<td align="right">0.9907</td>
<td align="right">0.9979</td>
</tr>
<tr class="even">
<td>Specificity</td>
<td align="right">0.9986</td>
<td align="right">0.9967</td>
<td align="right">0.9968</td>
<td align="right">0.9997</td>
<td align="right">1.0000</td>
</tr>
<tr class="odd">
<td>Pos Pred Value</td>
<td align="right">0.9964</td>
<td align="right">0.9862</td>
<td align="right">0.9846</td>
<td align="right">0.9984</td>
<td align="right">1.0000</td>
</tr>
<tr class="even">
<td>Neg Pred Value</td>
<td align="right">0.9998</td>
<td align="right">0.9978</td>
<td align="right">0.9966</td>
<td align="right">0.9982</td>
<td align="right">0.9995</td>
</tr>
<tr class="odd">
<td>Prevalence</td>
<td align="right">0.2845</td>
<td align="right">0.1935</td>
<td align="right">0.1744</td>
<td align="right">0.1639</td>
<td align="right">0.1838</td>
</tr>
<tr class="even">
<td>Detection Rate</td>
<td align="right">0.2843</td>
<td align="right">0.1917</td>
<td align="right">0.1716</td>
<td align="right">0.1624</td>
<td align="right">0.1834</td>
</tr>
<tr class="odd">
<td>Detection Prevalence</td>
<td align="right">0.2854</td>
<td align="right">0.1944</td>
<td align="right">0.1742</td>
<td align="right">0.1626</td>
<td align="right">0.1834</td>
</tr>
<tr class="even">
<td>Balanced Accuracy</td>
<td align="right">0.9991</td>
<td align="right">0.9937</td>
<td align="right">0.9903</td>
<td align="right">0.9952</td>
<td align="right">0.9990</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="predicting-on-the-test-dataset" class="section level2">
<h2>Predicting on the test Dataset</h2>
<p>Now I will use the two best performing algorithms to predict how well individuals perform the dumbbell exercises using the test dataset for submission.</p>
<ol start="9" style="list-style-type: decimal">
<li>Transforming the test dataset in the same way as training. Then using the Random Forest model for prediction</li>
</ol>
<pre class="r"><code>test   &lt;- test[,-nrzv]
test   &lt;- test[, -c(1:6)]
test   &lt;- test[, -xNAs]
test   &lt;- test[,-length(test)]
testRF &lt;- predict(rfm, newdata = test)</code></pre>
<ol start="10" style="list-style-type: decimal">
<li>Using the GBM model for prediction. Compare predictions</li>
</ol>
<pre class="r"><code>probs2   &lt;- predict(gbm1, test, n.trees = best.iter, type = &quot;response&quot;)
indexes2 &lt;- apply(probs2, 1, which.max)
testGBM  &lt;- as.factor(colnames(probs2)[indexes2])
answers  &lt;- cbind.data.frame(testRF, testGBM)
colnames(answers) &lt;- c(&quot;Random Forest:&quot;, &quot;Boosted (GBM)&quot;)
identical(answers[,1], answers[,2])</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>The RF and the GBM predictions are identical. The answers to be submitted are:</p>
<table class="kable_wrapper">
<caption>
<strong>Predictions on test Dataset</strong>
</caption>
<tbody>
<tr>
<td>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">1</th>
<th align="left">2</th>
<th align="left">3</th>
<th align="left">4</th>
<th align="left">5</th>
<th align="left">6</th>
<th align="left">7</th>
<th align="left">8</th>
<th align="left">9</th>
<th align="left">10</th>
<th align="left">11</th>
<th align="left">12</th>
<th align="left">13</th>
<th align="left">14</th>
<th align="left">15</th>
<th align="left">16</th>
<th align="left">17</th>
<th align="left">18</th>
<th align="left">19</th>
<th align="left">20</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Random Forest:</td>
<td align="left">B</td>
<td align="left">A</td>
<td align="left">B</td>
<td align="left">A</td>
<td align="left">A</td>
<td align="left">E</td>
<td align="left">D</td>
<td align="left">B</td>
<td align="left">A</td>
<td align="left">A</td>
<td align="left">B</td>
<td align="left">C</td>
<td align="left">B</td>
<td align="left">A</td>
<td align="left">E</td>
<td align="left">E</td>
<td align="left">A</td>
<td align="left">B</td>
<td align="left">B</td>
<td align="left">B</td>
</tr>
<tr class="even">
<td>Boosted (GBM)</td>
<td align="left">B</td>
<td align="left">A</td>
<td align="left">B</td>
<td align="left">A</td>
<td align="left">A</td>
<td align="left">E</td>
<td align="left">D</td>
<td align="left">B</td>
<td align="left">A</td>
<td align="left">A</td>
<td align="left">B</td>
<td align="left">C</td>
<td align="left">B</td>
<td align="left">A</td>
<td align="left">E</td>
<td align="left">E</td>
<td align="left">A</td>
<td align="left">B</td>
<td align="left">B</td>
<td align="left">B</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion:</h2>
<p>This analysis used accelerometer data to predict how well individuals perform dumbbell-lifting exercises. Three machine-learning algorithms were tested: CART, Random Forest, and GBM. The best performing algorithm was the Random Forest with <span class="math inline">\(99.3\%\)</span> accuracy, followed by the GBM with <span class="math inline">\(94.3\%\)</span> accuracy, and worst performing was the CART algorithm, which performed poorly compared to the other two with a <span class="math inline">\(72.4\%\)</span> accuracy.</p>
</div>
<div id="appendix" class="section level2">
<h2>Appendix</h2>
<p>Fig 1. Correlation matrix visualization</p>
<pre class="r"><code>par(xpd=TRUE)
corrplot.mixed(cor(training[,-length(training)]), lower=&quot;color&quot;, upper=&quot;circle&quot;, mar=c(1,1,1,1),
               tl.pos=&quot;lt&quot;, diag=&quot;n&quot;, title = &quot;Correlation Matrix Visualization&quot;,
               order=&quot;hclust&quot;, hclust.method=&quot;complete&quot;, tl.cex = .65, tl.col =&quot;#656565&quot;)</code></pre>
<p><img src="barbellRmd_files/figure-html/plotcorr-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>Fig 2. Rpart decision tree</p>
<pre class="r"><code>rpart.plot(rpart1, main=&quot;Decision Tree (rpart)&quot;, type = 1, extra=0, cex = NULL, 
           tweak = 1, fallen.leaves = FALSE, shadow.col = &quot;#e0e0e0&quot;, box.palette = mycolors)</code></pre>
<p><img src="barbellRmd_files/figure-html/plot1-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Fig 3. Random Forest oob error rate</p>
<pre class="r"><code>plot(gg_error(rfm)) + theme_bw() + scale_y_continuous(limits=c(0,.05)) + ggtitle(&quot;OOB Error Rate (Random Forest)&quot;) + geom_line(size=.75)</code></pre>
<p><img src="barbellRmd_files/figure-html/rf1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Fig 4. Variables of importance (Random Forest)</p>
<pre class="r"><code>vimp &lt;- varImp(rfm); vimp &lt;- cbind(measure = rownames(vimp), vimp)
vimp &lt;- arrange(vimp, desc(Overall))
vimp$measure &lt;- factor(vimp$measure, levels = vimp$measure)
ggplot(vimp[1:12,], aes(measure, Overall)) + theme_bw() +
        geom_bar(stat = &quot;identity&quot;, fill = &quot;#ff4d94&quot;, alpha = 0.8) +
        theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
        xlab(&quot;Measure&quot;) + 
        ggtitle(&quot;Top 12 Variables of Importance (Random Forest)&quot;)</code></pre>
<p><img src="barbellRmd_files/figure-html/rf2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Fig 5. Variables of importance (GBM)</p>
<pre class="r"><code>vimp2 &lt;- head(summary(gbm1, plotit = FALSE), 12)
vimp2$var &lt;- factor(vimp2$var, levels = vimp2$var)
ggplot(vimp2, aes(var, rel.inf)) + theme_bw() +
        geom_bar(stat = &quot;identity&quot;, fill = &quot;#b366ff&quot;, alpha = 0.8) +
        theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
        xlab(&quot;Measure&quot;) + 
        ggtitle(&quot;Top 12 Variables of Importance (GBM)&quot;)</code></pre>
<p><img src="barbellRmd_files/figure-html/gbm22-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><strong>Reference:</strong><br />
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. (2013) Qualitative Activity Recognition of Weight Lifting Exercises. <em>Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13)</em> . Stuttgart, Germany: ACM SIGCHI</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
